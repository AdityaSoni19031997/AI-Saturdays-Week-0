{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dd9fd08be61e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Result as a Tensor\n",
    "\n",
    "This will return the sum into result variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Place Addition\n",
    "\n",
    "In place functions are followed by an _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing, Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU\n",
    "Moving the Tensors to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7f417847cc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "autograd.Variable is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call .backward() and have all the gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the .data attribute, while the gradient w.r.t. this variable is accumulated into .grad.\n",
    "\n",
    "![](img/auto.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\",  4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyTorch Tutorial\n",
    "MILA, November 2017\n",
    "By Sandeep Subramanian\n",
    "1. Introduction to the torch tensor library\n",
    "Torch's numpy equivalent with GPU support\n",
    "In [2]:\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "In [3]:\n",
    "import torch\n",
    "Initialize a random tensor\n",
    "In [4]:\n",
    "torch.Tensor(5, 3)\n",
    "Out[4]:\n",
    " 2.4878e+04  4.5692e-41  2.4878e+04\n",
    " 4.5692e-41 -2.9205e+19  4.5691e-41\n",
    " 1.2277e-02  4.5692e-41 -4.0170e+19\n",
    " 4.5691e-41  1.2277e-02  4.5692e-41\n",
    " 0.0000e+00  0.0000e+00  0.0000e+00\n",
    "[torch.FloatTensor of size 5x3]\n",
    "From a uniform distribution\n",
    "In [5]:\n",
    "torch.Tensor(5, 3).uniform_(-1, 1)\n",
    "Out[5]:\n",
    "-0.2767 -0.1082 -0.1339\n",
    "-0.6477  0.3098  0.1642\n",
    "-0.1125 -0.2104  0.8962\n",
    "-0.6573  0.9669 -0.3806\n",
    " 0.8008 -0.3860  0.6816\n",
    "[torch.FloatTensor of size 5x3]\n",
    "Get it's shape\n",
    "In [6]:\n",
    "x = torch.Tensor(5, 3).uniform_(-1, 1)\n",
    "print(x.size())\n",
    "torch.Size([5, 3])\n",
    "Tensor Types\n",
    "source: http://pytorch.org/docs/master/tensors.html\n",
    "Data type\tTensor\n",
    "32-bit floating point\ttorch.FloatTensor\n",
    "64-bit floating point\ttorch.DoubleTensor\n",
    "16-bit floating point\ttorch.HalfTensor\n",
    "8-bit integer (unsigned)\ttorch.ByteTensor\n",
    "8-bit integer (signed)\ttorch.CharTensor\n",
    "16-bit integer (signed)\ttorch.ShortTensor\n",
    "32-bit integer (signed)\ttorch.IntTensor\n",
    "64-bit integer (signed)\ttorch.LongTensor\n",
    "Creation from lists & numpy\n",
    "In [7]:\n",
    "z = torch.LongTensor([[1, 3], [2, 9]])\n",
    "print(z.type())\n",
    "# Cast to numpy ndarray\n",
    "print(z.numpy().dtype)\n",
    "torch.LongTensor\n",
    "int64\n",
    "In [8]:\n",
    "# Data type inferred from numpy\n",
    "print(torch.from_numpy(np.random.rand(5, 3)).type())\n",
    "print(torch.from_numpy(np.random.rand(5, 3).astype(np.float32)).type())\n",
    "torch.DoubleTensor\n",
    "torch.FloatTensor\n",
    "Simple mathematical operations\n",
    "In [9]:\n",
    "y = x * torch.randn(5, 3)\n",
    "print(y)\n",
    " 0.2200 -0.0368  0.4494\n",
    "-0.2577 -0.0343  0.1587\n",
    "-0.7503 -0.1729  0.0453\n",
    " 0.9296 -0.1067 -0.6402\n",
    "-0.3276  0.0158 -0.0552\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "In [10]:\n",
    "y = x / torch.sqrt(torch.randn(5, 3) ** 2)\n",
    "print(y)\n",
    " 0.2820 -0.1633 -4.4346\n",
    "-1.6809  0.2066 -0.8261\n",
    "-0.6464  0.9758  0.2542\n",
    " 0.5789  0.1890 -0.4662\n",
    " 5.3183  0.0236 -0.1403\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "Broadcasting\n",
    "In [11]:\n",
    "print (x.size())\n",
    "y = x + torch.randn(5, 1)\n",
    "print(y)\n",
    "torch.Size([5, 3])\n",
    "\n",
    " 0.1919 -0.5006 -1.2410\n",
    "-0.8080  0.1407 -0.6193\n",
    "-1.6629 -0.1580 -0.3921\n",
    " 1.0395  0.7069 -0.1459\n",
    " 1.9027  1.4343  1.2299\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "Reshape\n",
    "In [12]:\n",
    "y = torch.randn(5, 10, 15)\n",
    "print(y.size())\n",
    "print(y.view(-1, 15).size())  # Same as doing y.view(50, 15)\n",
    "print(y.view(-1, 15).unsqueeze(1).size()) # Adds a dimension at index 1.\n",
    "print(y.view(-1, 15).unsqueeze(1).squeeze().size())\n",
    "# If input is of shape: (Ax1xBxCx1xD)(Ax1xBxCx1xD) then the out Tensor will be of shape: (AxBxCxD)(AxBxCxD)\n",
    "print()\n",
    "print(y.transpose(0, 1).size())\n",
    "print(y.transpose(1, 2).size())\n",
    "print(y.transpose(0, 1).transpose(1, 2).size())\n",
    "print(y.permute(1, 2, 0).size())\n",
    "torch.Size([5, 10, 15])\n",
    "torch.Size([50, 15])\n",
    "torch.Size([50, 1, 15])\n",
    "torch.Size([50, 15])\n",
    "\n",
    "torch.Size([10, 5, 15])\n",
    "torch.Size([5, 15, 10])\n",
    "torch.Size([10, 15, 5])\n",
    "torch.Size([10, 15, 5])\n",
    "Repeat\n",
    "In [13]:\n",
    "print(y.view(-1, 15).unsqueeze(1).expand(50, 100, 15).size())\n",
    "print(y.view(-1, 15).unsqueeze(1).expand_as(torch.randn(50, 100, 15)).size())\n",
    "torch.Size([50, 100, 15])\n",
    "torch.Size([50, 100, 15])\n",
    "Concatenate\n",
    "In [14]:\n",
    "# 2 is the dimension over which the tensors are concatenated\n",
    "print(torch.cat([y, y], 2).size())\n",
    "# stack concatenates the sequence of tensors along a new dimension.\n",
    "print(torch.stack([y, y], 0).size())\n",
    "torch.Size([5, 10, 30])\n",
    "torch.Size([2, 5, 10, 15])\n",
    "Advanced Indexing\n",
    "In [15]:\n",
    "y = torch.randn(2, 3, 4)\n",
    "print(y[[1, 0, 1, 1]].size())\n",
    "\n",
    "# PyTorch doesn't support negative strides yet so ::-1 does not work.\n",
    "rev_idx = torch.arange(1, -1, -1).long()\n",
    "print(y[rev_idx].size())\n",
    "torch.Size([4, 3, 4])\n",
    "torch.Size([2, 3, 4])\n",
    "GPU support\n",
    "In [16]:\n",
    "x = torch.cuda.HalfTensor(5, 3).uniform_(-1, 1)\n",
    "y = torch.cuda.HalfTensor(3, 5).uniform_(-1, 1)\n",
    "torch.matmul(x, y)\n",
    "Out[16]:\n",
    " 0.2456  1.1543  0.5376  0.4358 -0.0369\n",
    " 0.8247 -0.4143 -0.7188  0.3953  0.2573\n",
    "-0.1346  0.7329  0.5156  0.0864 -0.1349\n",
    "-0.3555  0.3135  0.3921 -0.1428 -0.1368\n",
    "-0.4385  0.5601  0.6533 -0.2793 -0.5220\n",
    "[torch.cuda.HalfTensor of size 5x5 (GPU 0)]\n",
    "Move tensors on the CPU -> GPU\n",
    "In [17]:\n",
    "x = torch.FloatTensor(5, 3).uniform_(-1, 1)\n",
    "print(x)\n",
    "x = x.cuda(device=0)\n",
    "print(x)\n",
    "x = x.cpu()\n",
    "print(x)\n",
    "-0.3758 -0.1090  0.7911\n",
    " 0.2839 -0.9136  0.1070\n",
    " 0.9184  0.5113 -0.8040\n",
    "-0.3412 -0.8895 -0.5780\n",
    "-0.0992  0.0983  0.6074\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "\n",
    "-0.3758 -0.1090  0.7911\n",
    " 0.2839 -0.9136  0.1070\n",
    " 0.9184  0.5113 -0.8040\n",
    "-0.3412 -0.8895 -0.5780\n",
    "-0.0992  0.0983  0.6074\n",
    "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
    "\n",
    "\n",
    "-0.3758 -0.1090  0.7911\n",
    " 0.2839 -0.9136  0.1070\n",
    " 0.9184  0.5113 -0.8040\n",
    "-0.3412 -0.8895 -0.5780\n",
    "-0.0992  0.0983  0.6074\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "Contiguity in memory\n",
    "In [18]:\n",
    "x = torch.FloatTensor(5, 3).uniform_(-1, 1)\n",
    "print(x)\n",
    "x = x.cuda(device=0)\n",
    "print(x)\n",
    "print('Contiguity : %s ' % (x.is_contiguous()))\n",
    "x = x.unsqueeze(0).expand(30, 5, 3)\n",
    "print('Contiguity : %s ' % (x.is_contiguous()))\n",
    "x = x.contiguous()\n",
    "print('Contiguity : %s ' % (x.is_contiguous()))\n",
    " 0.4740 -0.9209  0.4143\n",
    "-0.3473  0.4474 -0.8159\n",
    "-0.7654 -0.0956  0.6145\n",
    "-0.0846 -0.6239  0.8609\n",
    "-0.8142  0.9289 -0.7020\n",
    "[torch.FloatTensor of size 5x3]\n",
    "\n",
    "\n",
    " 0.4740 -0.9209  0.4143\n",
    "-0.3473  0.4474 -0.8159\n",
    "-0.7654 -0.0956  0.6145\n",
    "-0.0846 -0.6239  0.8609\n",
    "-0.8142  0.9289 -0.7020\n",
    "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
    "\n",
    "Contiguity : True \n",
    "Contiguity : False \n",
    "Contiguity : True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.IntTensor([2, 3, 4])\n",
    "b = torch.IntTensor([3, 4, 5])\n",
    "m = a * b  # element-wise product\n",
    "print(m.numpy())  # convert to the numpy array [ 6 12 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/0_multiply.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
